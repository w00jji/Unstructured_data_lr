# Unstructured_data

## 자연어(영어) 처리 과정 - 1010
1) 토큰화 : 자연어 데이터 분석을 위한 작은 단위(토큰)로 분리
2) 정제 : 분석에 큰 의미가 없는 데이터 제거
3) 정규화 : 표현 방법이 다르지만 의미가 같은 단어들을 통합
4) 정수 인코딩 : 컴퓨터가 이해하기 쉽도록 자연어 데이터에 정수 인덱스를 부여

- 사용기법 
1) 토큰화 : 단어 또는 문장 단위로 토큰화
2) 정제 : 빈도 분석 후 특정 단어 빈도 수 제거 -> 특정 단어 길이 제거 -> 불용어 처리 -> 원형 추출
3) 정규화 : 소문자 처리(정규화) -> 어간 추출(정규화)


## 감성분석 - 1011
- 정규화,토큰화,정규화, 라벨링
  - 라벨링 : 빈도가 많이 나온단어부터 라벨링 - 중요, 라벨링시 0부터 라벨링 X 1부터해야함
   - 이유 : 유사도를 구하기위해 길이가 같아야하고 길이를 늘렸을 때 빈 라벨을 0으로 채워주기 위함
 
- 감성분석
  - 절차 :
    - wordnet 사용하여 단어의 의미를 파악
    - 파악 후 단어의 점수를 매김 , 양수면 "긍정" 음수면 "부정"

  - 사용기법 : sentiWordNet , Vader
    - sentiWordNet : 상세한 감정 정보 제공 , 문맥을 고려하지 않음 , 사용이 다소 복잡 , **공식적인 문서에서 유용**
    - Vader : 사용하기 간편 , 문맥 정보 반영 , **소셜 미디어 데이터에 적합** , 영어에 최적화
      
    - 영어,한국어 - 감성분석
    - OpenAI API 활용하여 감성분석

## 빈도분석 - 1014
- 사용기법 : 빈도 수 가로 막대 그래프, 워드클라우드를 활용한 시각화
  - 절차 
    - 전처리 : 길이가 2이하인 단어 제거, 명사 추출
    - 시각화 : 가로 막대그래프 , 워드클라우드
    - TF 분석 : TF, DF, IDF, TF-IDF 생성 - 간단한 라이브러리 TfidfVectorizer -> 결과 : 값이 가장 큰걸 가져옴

    - 과제 : 대구,하나은행 기사 크롤링 후 빈도 분석
    - 사용 기법 :
      - 크롤링 : 셀레니움, bs4
      - 빈도 분석 : 위와 같음
        - 링크 : https://github.com/w00jji/Unstructured_data_lr/tree/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%A7%88%EC%9D%B4%EB%8B%9D1014_(%EB%B9%88%EB%8F%84%EB%B6%84%EC%84%9D)/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC_%EB%B6%84%EC%84%9D
     
